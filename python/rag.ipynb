{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.0.1)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.9.0.post1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\te\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Using cached sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Installing collected packages: transformers, sentence-transformers\n",
      "Successfully installed sentence-transformers-3.3.1 transformers-4.46.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\TE\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 sentence-transformers faiss-cpu transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, LlamaTokenizer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: reports\\202407_TE_advanced_biofuels_report.pdf\n",
      "System ready! You can now ask questions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Extract Text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    title = reader.metadata.get('/Title', None)  # Extract the title from metadata\n",
    "    title = title if title else \"Unknown Title\"\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text, title\n",
    "\n",
    "# Step 2: Chunk Text\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        yield \" \".join(words[i:i + chunk_size])\n",
    "\n",
    "# Step 3: Create Embeddings and FAISS Index\n",
    "def create_faiss_index(chunks, model):\n",
    "    embeddings = [model.encode(chunk) for chunk in chunks]\n",
    "    embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "\n",
    "    return index, embeddings\n",
    "\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\"\n",
    "    return query_ollama(prompt)\n",
    "\n",
    "# Step 4: Retrieve Relevant Chunks\n",
    "def retrieve(query, index, model, chunks, top_k=3):\n",
    "    query_embedding = model.encode(query).astype(\"float32\")\n",
    "    distances, indices = index.search(query_embedding.reshape(1, -1), top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "def retrieve_with_metadata(query, index, model, chunks, metadata, top_k=3):\n",
    "    query_embedding = model.encode(query).astype(\"float32\")\n",
    "    distances, indices = index.search(query_embedding.reshape(1, -1), top_k)\n",
    "    retrieved_chunks = [chunks[i] for i in indices[0]]\n",
    "    retrieved_metadata = [metadata[i] for i in indices[0]]\n",
    "    return retrieved_chunks, retrieved_metadata\n",
    "\n",
    "def truncate_context(context, query, max_tokens=2048):\n",
    "    # Reserve space for the query and additional prompt text\n",
    "    reserved_tokens = 300  # Adjust this based on the length of your query\n",
    "    max_context_tokens = max_tokens - reserved_tokens\n",
    "\n",
    "    # Truncate context to fit within the limit\n",
    "    context_tokens = context.split()  # Tokenize the context\n",
    "    if len(context_tokens) > max_context_tokens:\n",
    "        context = \" \".join(context_tokens[:max_context_tokens])\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "def query_ollama(prompt, model=\"llama3.2\", server_url=\"http://localhost:11435/api/generate\"):\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"model\": model, \"prompt\": prompt}\n",
    "\n",
    "    response = requests.post(server_url, headers=headers, json=payload, stream=True)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "    # Print and collect the streamed response\n",
    "    print(\"Response: \", end=\"\", flush=True)  # Start the response line\n",
    "    full_response = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line.decode(\"utf-8\"))\n",
    "            part = data.get(\"response\", \"\")\n",
    "            print(part, end=\"\", flush=True)  # Print the response part immediately\n",
    "            full_response += part\n",
    "            if data.get(\"done\", False):\n",
    "                break\n",
    "\n",
    "    print()  # Finish the response line\n",
    "    return full_response\n",
    "\n",
    "def summarize_context(context):\n",
    "    prompt = f\"Summarize the following text:\\n\\n{context}\"\n",
    "    return query_ollama(prompt)\n",
    "\n",
    "# Step 1: Extract text from multiple PDFs\n",
    "def process_multiple_pdfs(folder_path):\n",
    "    all_chunks = []\n",
    "    chunk_metadata = []  # Store metadata for each chunk\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing: {pdf_path}\")\n",
    "\n",
    "            # Extract text from the PDF\n",
    "            full_text, title = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            # Chunk the text\n",
    "            chunks = list(chunk_text(full_text))\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "            # Add metadata (file name) for each chunk\n",
    "            chunk_metadata.extend([{\"file_name\": filename, \"title\": title, \"page_number\": page_number} for page_number, chunk in enumerate(chunks)])\n",
    "\n",
    "    return all_chunks, chunk_metadata\n",
    "\n",
    "\n",
    "# Small and fast embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Folder containing multiple PDFs\n",
    "folder_path = \"reports\"  # Replace with your folder path\n",
    "\n",
    "# Process all PDFs in the folder\n",
    "all_chunks, chunk_metadata = process_multiple_pdfs(folder_path)\n",
    "\n",
    "# Step 3: Create FAISS index\n",
    "index, embeddings = create_faiss_index(all_chunks, embedding_model)\n",
    "\n",
    "\n",
    "print(\"System ready! You can now ask questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nLlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the tokenizer for your model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenlm-research/open_llama_3b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_tokens\u001b[39m(input_string):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Tokenize the input string\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     tokenized \u001b[38;5;241m=\u001b[39m tokenizer(input_string, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1651\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1651\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1639\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1637\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nLlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the tokenizer for your model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"openlm-research/open_llama_3b\")\n",
    "\n",
    "def count_tokens(input_string):\n",
    "    # Tokenize the input string\n",
    "    tokenized = tokenizer(input_string, truncation=False, return_tensors=\"pt\")\n",
    "    return len(tokenized[\"input_ids\"][0])  # Return the number of tokens\n",
    "\n",
    "def generate_answer_with_citation(query, chunks, metadata):\n",
    "    # Combine chunks into context\n",
    "    context = \"\\n\\n\".join(chunks)\n",
    "\n",
    "\n",
    "    # Measure total tokens in context and query\n",
    "    total_tokens = count_tokens(context) + count_tokens(query)\n",
    "    if total_tokens > max_tokens:\n",
    "        print(\"Truncating context ...\")\n",
    "        print(f\"Total tokens ({total_tokens}) exceed the limit ({max_tokens}). Truncating context...\")\n",
    "        context = truncate_context(context, query, max_tokens)\n",
    "\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {truncated_context}\"\n",
    "\n",
    "    print(\"Querying LLM with prompt: \", prompt)\n",
    "\n",
    "    # Generate answer using the model\n",
    "    answer = query_ollama(prompt)\n",
    "\n",
    "    # Add citations to the answer\n",
    "    citations = [f\"Source: {meta['file_name']}, Title: {meta['title']}, Page: {meta['page_number']}\" for meta in metadata]\n",
    "    citation_text = \"\\n\".join(citations)\n",
    "\n",
    "    return f\"{answer}\\n\\nCitations:\\n{citation_text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m retrieved_chunks, retrieved_metadata \u001b[38;5;241m=\u001b[39m retrieve_with_metadata(query, index, embedding_model, all_chunks, chunk_metadata)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Step 4: Generate answer with citations\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m answer_with_citation \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer_with_citation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer_with_citation)\n",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m, in \u001b[0;36mgenerate_answer_with_citation\u001b[1;34m(query, chunks, metadata)\u001b[0m\n\u001b[0;32m      8\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Measure total tokens in context and query\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m total_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mcount_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m count_tokens(query)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_tokens \u001b[38;5;241m>\u001b[39m max_tokens:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTruncating context ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m, in \u001b[0;36mcount_tokens\u001b[1;34m(input_string)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_tokens\u001b[39m(input_string):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Tokenize the input string\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(input_string, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokenized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#query = input(\"\\nEnter your question (or 'exit' to quit): \")\n",
    "\n",
    "# Step 3: Query and retrieve relevant chunks with metadata\n",
    "query = r\" Is the statement true or false?  @ FuelsEurope @ @FuelsEurope - Oct 22, 2021\\\n",
    "\\\n",
    "Asstudy from @imperialcollege shows there is sufficient sustainable\\\n",
    "biomass feedstock available to support an ambitious\\\n",
    "#lowcarbonliquidfuels strategy for EU transport. Read @DG FuelsEurope\\\n",
    "op-ed in @POLITICOEurope today: politi.co/3E4vkoF\\\n",
    "\\\n",
    "#CleanFuelsforAll\\\n",
    "\\\n",
    "om politico.eu\\\n",
    "\\\n",
    "O18\\\n",
    "\\\n",
    "lable to support low-carbon liquid fuels in the EU\\\n",
    "v a |\\\n",
    "\\\n",
    "ti 120 9 470 nn\"\n",
    "retrieved_chunks, retrieved_metadata = retrieve_with_metadata(query, index, embedding_model, all_chunks, chunk_metadata)\n",
    "\n",
    "# Step 4: Generate answer with citations\n",
    "answer_with_citation = generate_answer_with_citation(query, retrieved_chunks, retrieved_metadata)\n",
    "print(answer_with_citation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
